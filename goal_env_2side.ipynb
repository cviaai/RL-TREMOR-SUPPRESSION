{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "goal_env_2side.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsRylem8SJ6R",
        "colab_type": "code",
        "colab": {},
        "outputId": "215e5158-ddb6-4152-aafd-b9a96f15c049"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import serial\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import datetime \n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common import make_vec_env\n",
        "from stable_baselines import A2C, PPO2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import serial\n",
        "import csv\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5-jsO8cSJ6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class handGoalEnv(gym.Env):\n",
        "    \"\"\"A goal-based environment. It functions just as any regular OpenAI Gym environment but it\n",
        "    imposes a required structure on the observation_space. More concretely, the observation\n",
        "    space is required to contain at least three elements, namely `observation`, `desired_goal`, and\n",
        "    `achieved_goal`. Here, `desired_goal` specifies the goal that the agent should attempt to achieve.\n",
        "    `achieved_goal` is the goal that it currently achieved instead. `observation` contains the\n",
        "    actual observations of the environment as per usual.\n",
        "    \"\"\"\n",
        "    \n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, arduino_port_motor, arduino_port_sensor, time_observation = 1000, threshold = 5000):\n",
        "        super(handGoalEnv, self).__init__()\n",
        "             \n",
        "        self.arduino_port_motor = arduino_port_motor\n",
        "        self.arduino_port_sensor = arduino_port_sensor\n",
        "\n",
        "        self.time_observation = time_observation\n",
        "\n",
        "        self.action_space = spaces.Discrete(511)\n",
        "        self.observation_space = spaces.Box(low=-10e+4, high=10e+4, \n",
        "                                             shape=(3,time_observation), \n",
        "                                             dtype=np.float16)\n",
        "    \n",
        "        #self.goal_obs = 200#self.compute_noise() #level of noise\n",
        "        self.threshold = threshold #the threshold after which a goal is considered achieved\n",
        "        \n",
        "        self.number_of_episodes = 0\n",
        "        self.done_moment = 0\n",
        "        \n",
        "        self.sped = 250\n",
        "        \n",
        "    '''   \n",
        "    def compute_noise(self):\n",
        "        self.motor(0)\n",
        "        noise = self._next_observation()\n",
        "        noise_lvl = self.compute_reward(noise)\n",
        "        print('noise lvl:', noise_lvl)\n",
        "        return noise_lvl\n",
        "    '''     \n",
        "    def reset(self):\n",
        "        \n",
        "        # Reset the state of the environment to an initial state\n",
        "        print('Reset')\n",
        "        \n",
        "        print('done_moment:', self.done_moment)\n",
        "        \n",
        "        self.done_moment = 0\n",
        "        \n",
        "#         if self.done_max < self.done_moment:\n",
        "#             self.done_max = self.done_moment\n",
        "        \n",
        "        speed = 250\n",
        "        self.motor(speed, feedback = True)\n",
        "        \n",
        "        return self.sensors(rse = True)\n",
        "        \n",
        "        # Enforce that each GoalEnv uses a Goal-compatible observation space.\n",
        "#         if not isinstance(self.observation_space, gym.spaces.Dict):\n",
        "#             raise error.Error('GoalEnv requires an observation space of type gym.spaces.Dict')\n",
        "#         for key in ['observation', 'achieved_goal', 'desired_goal']:\n",
        "#             if key not in self.observation_space.spaces:\n",
        "#                 raise error.Error('GoalEnv requires the \"{}\" key to be part of the observation dictionary.'.format(key))\n",
        "\n",
        "    def compute_reward(self, obs):#achieved_goal, desired_goal, info):\n",
        "        \"\"\"Compute the step reward. This externalizes the reward function and makes\n",
        "        it dependent on an a desired goal and the one that was achieved. If you wish to include\n",
        "        additional rewards that are independent of the goal, you can include the necessary values\n",
        "        to derive it in info and compute it accordingly.\n",
        "        Args:\n",
        "            achieved_goal (object): the goal that was achieved during execution\n",
        "            desired_goal (object): the desired goal that we asked the agent to attempt to achieve\n",
        "            info (dict): an info dictionary with additional information\n",
        "        Returns:\n",
        "            float: The reward that corresponds to the provided achieved goal w.r.t. to the desired\n",
        "            goal. Note that the following should always hold true:\n",
        "                ob, reward, done, info = env.step()\n",
        "                assert reward == env.compute_reward(ob['achieved_goal'], ob['goal'], info)\n",
        "        \"\"\"\n",
        "        vector_mean_x = np.full( len(obs[0])  ,np.mean(obs[0]))\n",
        "        vector_mean_y = np.full( len(obs[1])  ,np.mean(obs[1]))\n",
        "        vector_mean_z = np.full( len(obs[2])  ,np.mean(obs[2]))\n",
        "        \n",
        "        x_new = np.sum((np.subtract(obs[0], vector_mean_x))**2)\n",
        "        y_new = np.sum((np.subtract(obs[1], vector_mean_y))**2)\n",
        "        z_new = np.sum((np.subtract(obs[2], vector_mean_z))**2)\n",
        "        \n",
        "        \n",
        "        return - np.sum([x_new, y_new, z_new])/3\n",
        "#         # Compute distance between goal and the achieved goal.\n",
        "#         d = goal_distance(achieved_goal, goal) #np.linalg.norm(achieved_goal - goal, axis=-1)\n",
        "#         return -d\n",
        "        \n",
        "    def step(self, action):\n",
        "        # Execute one time step within the environment\n",
        "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
        "        episode is reached, you are responsible for calling `reset()`\n",
        "        to reset this environment's state.\n",
        "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
        "        Args:\n",
        "            action (object): an action provided by the agent\n",
        "        Returns:\n",
        "            observation (object): agent's observation of the current environment\n",
        "            reward (float) : amount of reward returned after previous action\n",
        "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
        "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
        "        \"\"\"\n",
        "        self.number_of_episodes += 1\n",
        "        \n",
        "        if self.number_of_episodes%10 == 0:\n",
        "            print('==================================================')\n",
        "            print('Episode:', self.number_of_episodes)\n",
        "            self.motor(speed = 0)\n",
        "            time.sleep(50)\n",
        "#             self.goal_obs = self.compute_noise()\n",
        "#             time.sleep(20)\n",
        "            print('==================================================')\n",
        "        \n",
        "        print('____________________________________________')\n",
        "        print('action =', action)\n",
        "        self.sped = action\n",
        "        \n",
        "        self.motor(action)\n",
        "        time.sleep(10)\n",
        "        \n",
        "        obs = self._next_observation()\n",
        "        \n",
        "        rew = self.compute_reward(obs)\n",
        "        \n",
        "        distance = abs(rew) #- self.goal_obs) \n",
        "        \n",
        "        done = bool(distance < self.threshold)\n",
        "        \n",
        "        print('reward:', rew)\n",
        "        print('distance:', distance, 'done:', done)\n",
        "        \n",
        "        reward = rew\n",
        "        \n",
        "        if not done:\n",
        "            self.done_moment += 1\n",
        "        \n",
        "        return obs, reward, done, {}\n",
        "    \n",
        "    \n",
        "    def motor(self, speed, feedback = True):\n",
        "        self.arduino_port_motor.write(str(speed).encode())\n",
        "        if feedback == True:\n",
        "            print (\"RECIEVED BACK:\",self.arduino_port_motor.readline().decode())\n",
        "    \n",
        "    \n",
        "    def sensors(self, rse = False):\n",
        "        \n",
        "        if rse == False:\n",
        "        \n",
        "            column_names = ['x', 'y', 'z']\n",
        "            file_name =  '9v' +'_' + str(self.sped) + '.csv'\n",
        "            file_ = open(file_name, \"w\")\n",
        "            writer = csv.writer(file_, delimiter = \",\")\n",
        "            writer.writerow(column_names)\n",
        "        else:\n",
        "            print('reset')\n",
        "        \n",
        "        self.arduino_port_sensor.reset_input_buffer()\n",
        "        self.arduino_port_sensor.reset_output_buffer()\n",
        "        self.arduino_port_sensor.write(str(1).encode())\n",
        "        #print(arduino_port_sensor.readline())\n",
        "        command = self.arduino_port_sensor.readline().decode().rstrip().replace(\" \", \"\")\n",
        "        print('comand:', command)\n",
        "        main_array = [command]\n",
        "        #pbar = tqdm(total=self.time_observation)\n",
        "        #self.time_start = str(datetime.now())[14:-3]\n",
        "        while True:\n",
        "            values = self.arduino_port_sensor.readline().decode().rstrip().replace(\" \", \"\")\n",
        "            #print('values:', values)\n",
        "            main_array.append(values)\n",
        "            #pbar.update(1)\n",
        "            if values == '':\n",
        "                print('!!!!')\n",
        "                continue\n",
        "                #self.sensors()\n",
        "            if values == '0':\n",
        "                #self.arduino_port_sensor.write(str(0).encode())\n",
        "                break\n",
        "       \n",
        "        #self.time_end = str(datetime.now())[14:-3]\n",
        "        \n",
        "        #pbar.close()\n",
        "        #print('len:', len(main_array))\n",
        "        main_array = main_array[1:len(main_array)-1]\n",
        "        x = []\n",
        "        y = []\n",
        "        z = []\n",
        "        #print(main_array)   \n",
        "        for each_str in main_array:\n",
        "            result = [x.strip() for x in each_str.split(',')]\n",
        "            #print(result)\n",
        "            x.append(float(result[0]))\n",
        "            y.append(float(result[1]))\n",
        "            z.append(float(result[2]))\n",
        "            if rse == False:\n",
        "                writer.writerow([float(result[0]), float(result[1]), float(result[2])])\n",
        "            \n",
        "        if rse == False:\n",
        "            file_.close()\n",
        "        #print('ok')\n",
        "            \n",
        "        return [x, y, z]\n",
        "\n",
        "    \n",
        "    def _next_observation(self):\n",
        "        return self.sensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "v_vrwN_9SJ6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arduino_port_motor = serial.Serial('/dev/ttyACM4', 9600)\n",
        "arduino_port_motor = serial.Serial('/dev/ttyACM4', 9600)\n",
        "arduino_port_sensor = serial.Serial('/dev/ttyACM0', 9600, \n",
        "                                    timeout = 1)\n",
        "\n",
        "env = handGoalEnv(arduino_port_motor , arduino_port_sensor)\n",
        "\n",
        "#model = PPO2(MlpPolicy, env, tensorboard_log=\"./loser_tensorboard/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2e7TKf3cSJ6s",
        "colab_type": "code",
        "colab": {},
        "outputId": "50b5b28d-e0bf-4ffb-800d-8d7214d07be7"
      },
      "source": [
        "model.learn(total_timesteps=500, tb_log_name=\"7.5v\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reset\n",
            "done_moment: 0\n",
            "RECIEVED BACK: 250\n",
            "\n",
            "reset\n",
            "comand: 1\n",
            "____________________________________________\n",
            "action = 360\n",
            "RECIEVED BACK: 360\n",
            "\n",
            "comand: 1\n",
            "reward: -23543.887\n",
            "distance: 23543.887 done: False\n",
            "____________________________________________\n",
            "action = 49\n",
            "RECIEVED BACK: 49\n",
            "\n",
            "comand: 1\n",
            "reward: -25621.47566666667\n",
            "distance: 25621.47566666667 done: False\n",
            "____________________________________________\n",
            "action = 311\n",
            "RECIEVED BACK: 311\n",
            "\n",
            "comand: 1\n",
            "reward: -26747.150666666672\n",
            "distance: 26747.150666666672 done: False\n",
            "____________________________________________\n",
            "action = 430\n",
            "RECIEVED BACK: 430\n",
            "\n",
            "comand: 1\n",
            "reward: -147615.12600000002\n",
            "distance: 147615.12600000002 done: False\n",
            "____________________________________________\n",
            "action = 16\n",
            "RECIEVED BACK: 16\n",
            "\n",
            "comand: 1\n",
            "reward: -22578.862000000005\n",
            "distance: 22578.862000000005 done: False\n",
            "____________________________________________\n",
            "action = 324\n",
            "RECIEVED BACK: 324\n",
            "\n",
            "comand: 1\n",
            "reward: -22986.79533333333\n",
            "distance: 22986.79533333333 done: False\n",
            "____________________________________________\n",
            "action = 155\n",
            "RECIEVED BACK: 155\n",
            "\n",
            "comand: 1\n",
            "reward: -21867.588999999996\n",
            "distance: 21867.588999999996 done: False\n",
            "____________________________________________\n",
            "action = 83\n",
            "RECIEVED BACK: 83\n",
            "\n",
            "comand: 1\n",
            "reward: -22906.518999999997\n",
            "distance: 22906.518999999997 done: False\n",
            "____________________________________________\n",
            "action = 72\n",
            "RECIEVED BACK: 72\n",
            "\n",
            "comand: 1\n",
            "reward: -23816.47533333334\n",
            "distance: 23816.47533333334 done: False\n",
            "==================================================\n",
            "Episode: 10\n",
            "RECIEVED BACK: 0\n",
            "\n",
            "==================================================\n",
            "____________________________________________\n",
            "action = 371\n",
            "RECIEVED BACK: 371\n",
            "\n",
            "comand: 1\n",
            "reward: -24754.531666666673\n",
            "distance: 24754.531666666673 done: False\n",
            "____________________________________________\n",
            "action = 198\n",
            "RECIEVED BACK: 198\n",
            "\n",
            "comand: 1\n",
            "reward: -324613.5806666667\n",
            "distance: 324613.5806666667 done: False\n",
            "____________________________________________\n",
            "action = 400\n",
            "RECIEVED BACK: 400\n",
            "\n",
            "comand: 1\n",
            "reward: -19750.981999999993\n",
            "distance: 19750.981999999993 done: False\n",
            "____________________________________________\n",
            "action = 253\n",
            "RECIEVED BACK: 253\n",
            "\n",
            "comand: 1\n",
            "reward: -178595.005\n",
            "distance: 178595.005 done: False\n",
            "____________________________________________\n",
            "action = 200\n",
            "RECIEVED BACK: 200\n",
            "\n",
            "comand: 1\n",
            "reward: -188035.629\n",
            "distance: 188035.629 done: False\n",
            "____________________________________________\n",
            "action = 98\n",
            "RECIEVED BACK: 98\n",
            "\n",
            "comand: 1\n",
            "reward: -16674.875333333333\n",
            "distance: 16674.875333333333 done: False\n",
            "____________________________________________\n",
            "action = 445\n",
            "RECIEVED BACK: 445\n",
            "\n",
            "comand: 1\n",
            "reward: -160020.77599999998\n",
            "distance: 160020.77599999998 done: False\n",
            "____________________________________________\n",
            "action = 428\n",
            "RECIEVED BACK: 428\n",
            "\n",
            "comand: 1\n",
            "reward: -230032.65133333334\n",
            "distance: 230032.65133333334 done: False\n",
            "____________________________________________\n",
            "action = 69\n",
            "RECIEVED BACK: 69\n",
            "\n",
            "comand: 1\n",
            "reward: -14627.295333333333\n",
            "distance: 14627.295333333333 done: False\n",
            "____________________________________________\n",
            "action = 100\n",
            "RECIEVED BACK: 100\n",
            "\n",
            "comand: 1\n",
            "reward: -15841.341999999999\n",
            "distance: 15841.341999999999 done: False\n",
            "==================================================\n",
            "Episode: 20\n",
            "RECIEVED BACK: 0\n",
            "\n",
            "==================================================\n",
            "____________________________________________\n",
            "action = 316\n",
            "RECIEVED BACK: 316\n",
            "\n",
            "comand: 1\n",
            "reward: -15933.256999999998\n",
            "distance: 15933.256999999998 done: False\n",
            "____________________________________________\n",
            "action = 132\n",
            "RECIEVED BACK: 132\n",
            "\n",
            "comand: 1\n",
            "reward: -32504.205999999995\n",
            "distance: 32504.205999999995 done: False\n",
            "____________________________________________\n",
            "action = 12\n",
            "RECIEVED BACK: 12\n",
            "\n",
            "comand: 1\n",
            "reward: -14815.111333333336\n",
            "distance: 14815.111333333336 done: False\n",
            "____________________________________________\n",
            "action = 478\n",
            "RECIEVED BACK: 478\n",
            "\n",
            "comand: 1\n",
            "reward: -144389.1113333333\n",
            "distance: 144389.1113333333 done: False\n",
            "____________________________________________\n",
            "action = 366\n",
            "RECIEVED BACK: 366\n",
            "\n",
            "comand: 1\n",
            "reward: -14928.506333333331\n",
            "distance: 14928.506333333331 done: False\n",
            "____________________________________________\n",
            "action = 327\n",
            "RECIEVED BACK: 327\n",
            "\n",
            "comand: 1\n",
            "reward: -15032.63966666667\n",
            "distance: 15032.63966666667 done: False\n",
            "____________________________________________\n",
            "action = 57\n",
            "RECIEVED BACK: 57\n",
            "\n",
            "comand: 1\n",
            "reward: -16025.986333333332\n",
            "distance: 16025.986333333332 done: False\n",
            "____________________________________________\n",
            "action = 402\n",
            "RECIEVED BACK: 402\n",
            "\n",
            "comand: 1\n",
            "reward: -14736.779666666667\n",
            "distance: 14736.779666666667 done: False\n",
            "____________________________________________\n",
            "action = 223\n",
            "RECIEVED BACK: 223\n",
            "\n",
            "comand: 1\n",
            "reward: -133071.33666666667\n",
            "distance: 133071.33666666667 done: False\n",
            "____________________________________________\n",
            "action = 509\n",
            "RECIEVED BACK: 509\n",
            "\n",
            "comand: 1\n",
            "reward: -102108.418\n",
            "distance: 102108.418 done: False\n",
            "==================================================\n",
            "Episode: 30\n",
            "RECIEVED BACK: 0\n",
            "\n",
            "==================================================\n",
            "____________________________________________\n",
            "action = 356\n",
            "RECIEVED BACK: 356\n",
            "\n",
            "comand: 1\n",
            "reward: -15912.107333333333\n",
            "distance: 15912.107333333333 done: False\n",
            "____________________________________________\n",
            "action = 54\n",
            "RECIEVED BACK: 54\n",
            "\n",
            "comand: 1\n",
            "reward: -16220.601333333332\n",
            "distance: 16220.601333333332 done: False\n",
            "____________________________________________\n",
            "action = 216\n",
            "RECIEVED BACK: 216\n",
            "\n",
            "comand: 1\n",
            "reward: -129673.04233333335\n",
            "distance: 129673.04233333335 done: False\n",
            "____________________________________________\n",
            "action = 300\n",
            "RECIEVED BACK: 300\n",
            "\n",
            "comand: 1\n",
            "reward: -15533.956999999997\n",
            "distance: 15533.956999999997 done: False\n",
            "____________________________________________\n",
            "action = 318\n",
            "RECIEVED BACK: 318\n",
            "\n",
            "comand: 1\n",
            "reward: -16104.064666666667\n",
            "distance: 16104.064666666667 done: False\n",
            "____________________________________________\n",
            "action = 433\n",
            "RECIEVED BACK: 433\n",
            "\n",
            "comand: 1\n",
            "reward: -14887.091333333337\n",
            "distance: 14887.091333333337 done: False\n",
            "____________________________________________\n",
            "action = 263\n",
            "RECIEVED BACK: 263\n",
            "\n",
            "comand: 1\n",
            "reward: -16752.721\n",
            "distance: 16752.721 done: False\n",
            "____________________________________________\n",
            "action = 129\n",
            "RECIEVED BACK: 129\n",
            "\n",
            "comand: 1\n",
            "reward: -59210.303333333344\n",
            "distance: 59210.303333333344 done: False\n",
            "____________________________________________\n",
            "action = 292\n",
            "RECIEVED BACK: 292\n",
            "\n",
            "comand: 1\n",
            "reward: -19704.849\n",
            "distance: 19704.849 done: False\n",
            "____________________________________________\n",
            "action = 371\n",
            "RECIEVED BACK: 371\n",
            "\n",
            "comand: 1\n",
            "reward: -22362.574333333334\n",
            "distance: 22362.574333333334 done: False\n",
            "==================================================\n",
            "Episode: 40\n",
            "RECIEVED BACK: 0\n",
            "\n",
            "==================================================\n",
            "____________________________________________\n",
            "action = 215\n",
            "RECIEVED BACK: 215\n",
            "\n",
            "comand: 1\n",
            "reward: -193543.91566666667\n",
            "distance: 193543.91566666667 done: False\n",
            "____________________________________________\n",
            "action = 325\n",
            "RECIEVED BACK: 325\n",
            "\n",
            "comand: 1\n",
            "reward: -15396.941333333334\n",
            "distance: 15396.941333333334 done: False\n",
            "____________________________________________\n",
            "action = 381\n",
            "RECIEVED BACK: 381\n",
            "\n",
            "comand: 1\n",
            "reward: -15077.35\n",
            "distance: 15077.35 done: False\n",
            "____________________________________________\n",
            "action = 11\n",
            "RECIEVED BACK: 11\n",
            "\n",
            "comand: 1\n",
            "reward: -15463.561666666666\n",
            "distance: 15463.561666666666 done: False\n",
            "____________________________________________\n",
            "action = 142\n",
            "RECIEVED BACK: 142\n",
            "\n",
            "comand: 1\n",
            "reward: -13648.106\n",
            "distance: 13648.106 done: False\n",
            "____________________________________________\n",
            "action = 402\n",
            "RECIEVED BACK: 402\n",
            "\n",
            "comand: 1\n",
            "reward: -13426.698000000002\n",
            "distance: 13426.698000000002 done: False\n",
            "____________________________________________\n",
            "action = 404\n",
            "RECIEVED BACK: 404\n",
            "\n",
            "comand: 1\n",
            "reward: -13752.769999999999\n",
            "distance: 13752.769999999999 done: False\n",
            "____________________________________________\n",
            "action = 278\n",
            "RECIEVED BACK: 278\n",
            "\n",
            "comand: 1\n",
            "reward: -15651.070000000002\n",
            "distance: 15651.070000000002 done: False\n",
            "____________________________________________\n",
            "action = 392\n",
            "RECIEVED BACK: 392\n",
            "\n",
            "comand: 1\n",
            "reward: -14541.50366666667\n",
            "distance: 14541.50366666667 done: False\n",
            "____________________________________________\n",
            "action = 208\n",
            "RECIEVED BACK: 208\n",
            "\n",
            "comand: 1\n",
            "reward: -154772.42866666667\n",
            "distance: 154772.42866666667 done: False\n",
            "==================================================\n",
            "Episode: 50\n",
            "RECIEVED BACK: 0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-9195ec6850b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"7.5v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mcliprange_vf_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcliprange_vf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mep_info_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mmaybe_ep_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-5c2210f2c670>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;31m#             self.goal_obs = self.compute_noise()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m#             time.sleep(20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0B7fVTEcSJ6z",
        "colab_type": "code",
        "colab": {},
        "outputId": "2810ac81-1c3b-4894-d79a-6f53d4a249f2"
      },
      "source": [
        "model.learn(total_timesteps=500, tb_log_name=\"4.5v\", reset_num_timesteps=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reset\n",
            "done_moment: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-1c8889dc3f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"4.5v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, model, n_steps, gamma, lam)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mFactor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrade\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moff\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbias\u001b[0m \u001b[0mvs\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mGeneralized\u001b[0m \u001b[0mAdvantage\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \"\"\"\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, model, n_steps)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_ob_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_env\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_from_buf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-9e3bc6fbb912>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-9e3bc6fbb912>\u001b[0m in \u001b[0;36mmotor\u001b[0;34m(self, speed, feedback)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marduino_port_motor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeedback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"RECIEVED BACK:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marduino_port_motor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/serial/serialposix.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_abort_read_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_abort_read_r\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_abort_read_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ojYOxwM6SJ7B",
        "colab_type": "code",
        "colab": {},
        "outputId": "144a0a5c-5e80-49f9-c59c-a936544d5bfb"
      },
      "source": [
        "model.learn(total_timesteps=500, tb_log_name=\"train2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reset\n",
            "done_moment: 5\n",
            "RECIEVED BACK: 250\n",
            "\n",
            "comand: 1\n",
            "____________________________________________\n",
            "action = 32\n",
            "RECIEVED BACK: 32\n",
            "\n",
            "comand: 1\n",
            "reward: -837.5969999999999\n",
            "distance: 1037.5969999999998 done: False\n",
            "____________________________________________\n",
            "action = 485\n",
            "RECIEVED BACK: 485\n",
            "\n",
            "comand: 1\n",
            "reward: -60185.543000000005\n",
            "distance: 60385.543000000005 done: False\n",
            "____________________________________________\n",
            "action = 162\n",
            "RECIEVED BACK: 162\n",
            "\n",
            "comand: 1\n",
            "reward: -67730.11933333332\n",
            "distance: 67930.11933333332 done: False\n",
            "____________________________________________\n",
            "action = 441\n",
            "RECIEVED BACK: 441\n",
            "\n",
            "comand: 1\n",
            "reward: -60694.90533333333\n",
            "distance: 60894.90533333333 done: False\n",
            "____________________________________________\n",
            "action = 189\n",
            "RECIEVED BACK: 189\n",
            "\n",
            "comand: 1\n",
            "reward: -65771.93933333333\n",
            "distance: 65971.93933333333 done: False\n",
            "____________________________________________\n",
            "action = 354\n",
            "RECIEVED BACK: 354\n",
            "\n",
            "comand: 1\n",
            "reward: -6340.630333333332\n",
            "distance: 6540.630333333332 done: False\n",
            "____________________________________________\n",
            "action = 401\n",
            "RECIEVED BACK: 401\n",
            "\n",
            "comand: 1\n",
            "reward: -69116.44466666666\n",
            "distance: 69316.44466666666 done: False\n",
            "____________________________________________\n",
            "action = 472\n",
            "RECIEVED BACK: 472\n",
            "\n",
            "comand: 1\n",
            "reward: -60324.91266666667\n",
            "distance: 60524.91266666667 done: False\n",
            "____________________________________________\n",
            "action = 473\n",
            "RECIEVED BACK: 473\n",
            "\n",
            "comand: 1\n",
            "reward: -59956.599666666676\n",
            "distance: 60156.599666666676 done: False\n",
            "==================================================\n",
            "Episode: 130\n",
            "RECIEVED BACK: 0\n",
            "\n",
            "==================================================\n",
            "____________________________________________\n",
            "action = 311\n",
            "RECIEVED BACK: 311\n",
            "\n",
            "comand: 1\n",
            "reward: -1523.398\n",
            "distance: 1723.398 done: False\n",
            "____________________________________________\n",
            "action = 233\n",
            "RECIEVED BACK: 233\n",
            "\n",
            "comand: 1\n",
            "reward: -63347.38\n",
            "distance: 63547.38 done: False\n",
            "____________________________________________\n",
            "action = 289\n",
            "RECIEVED BACK: 289\n",
            "\n",
            "comand: 1\n",
            "reward: -852.8739999999999\n",
            "distance: 1052.8739999999998 done: False\n",
            "____________________________________________\n",
            "action = 326\n",
            "RECIEVED BACK: 326\n",
            "\n",
            "comand: 1\n",
            "reward: -2318.4379999999996\n",
            "distance: 2518.4379999999996 done: False\n",
            "____________________________________________\n",
            "action = 85\n",
            "RECIEVED BACK: 85\n",
            "\n",
            "comand: 1\n",
            "reward: -3108.292\n",
            "distance: 3308.292 done: False\n",
            "____________________________________________\n",
            "action = 313\n",
            "RECIEVED BACK: 313\n",
            "\n",
            "comand: 1\n",
            "reward: -1185.6273333333334\n",
            "distance: 1385.6273333333334 done: False\n",
            "____________________________________________\n",
            "action = 175\n",
            "RECIEVED BACK: 175\n",
            "\n",
            "comand: 1\n",
            "reward: -67462.761\n",
            "distance: 67662.761 done: False\n",
            "____________________________________________\n",
            "action = 62\n",
            "RECIEVED BACK: 62\n",
            "\n",
            "comand: 1\n",
            "reward: -2058.1446666666666\n",
            "distance: 2258.1446666666666 done: False\n",
            "____________________________________________\n",
            "action = 212\n",
            "RECIEVED BACK: 212\n",
            "\n",
            "comand: 1\n",
            "reward: -62980.218\n",
            "distance: 63180.218 done: False\n",
            "____________________________________________\n",
            "action = 77\n",
            "RECIEVED BACK: 77\n",
            "\n",
            "comand: 1\n",
            "reward: -2481.3163333333337\n",
            "distance: 2681.3163333333337 done: False\n",
            "==================================================\n",
            "Episode: 140\n",
            "RECIEVED BACK: 0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b90f19ac10f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mcliprange_vf_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcliprange_vf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mep_info_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mmaybe_ep_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c252917f2594>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;31m#             self.goal_obs = self.compute_noise()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m#             time.sleep(20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2kCHfq_SJ7D",
        "colab_type": "code",
        "colab": {},
        "outputId": "15805ad3-83ce-4203-98c4-f864070672e6"
      },
      "source": [
        "arduino_port_motor.write(str(0).encode())\n",
        "print(\"RECIEVED BACK:\",arduino_port_motor.readline().decode())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RECIEVED BACK: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r994g05SJ7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}